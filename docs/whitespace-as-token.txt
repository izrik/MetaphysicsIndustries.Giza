
Currently, we have a 'mind whitespace' directive. When combined with atomic, it
causes tokens to split on whitespace. The intersection of lexing and the
processing of whitespace is confusing at times, and makes implementing other
parts of the parsing re-architecting difficult. Theoretically, an atomic token
is supposed to end when a whitespace character is encountered. However, this
involves special logic, and is difficult to achieve within the ParserBase<T>
paradigm. If, instead of looking for whitespace specifically, we just treated
whitespace as a different kind of token, that would simplify a lot of things.
the boundary between a token of one definition and the whitespace that follows
it would become simply a boundary between tokens of different definitions,
which is already addressed.

A second problem with 'mind whitespace' is that the notion of "whitespace" is
dependent upon the platform's implementation of the `char.IsWhitepsace` method.
If an application/language wanted to consider a different set of characters as
whitespace, there is currently no way to override it.

Another problem is that definitions which explicitly include whitespace in them
must be marked as <mind whitespace> or they will never match the input, since
the incoming whitespace would be discarded. This isn't a technical problem, but
does increase cognitive load on users by annoyingly requiring them to remember
yet another thing.

Design proposal:
Altogether, it seems that it would be good to do one of the following things:
 * Introduce a 'whitespace' directive, which explicitly marks one definition as
   that which constitutes whitespace within the input domain. Likely, we would
   only allow one such definition, but not necessarilly; it just needs to be
   specified. If no definition was so marked, then a default would be included
   in the grammar, probably with the name '$whitespace', which would act
   exactly as the `char.IsWhitespace` method does now.
 * Introduce a directive that indicates that the token definition is to be
   ignored in input, and not passed from the lexing phase to the parsing phase.
   This would not necessarilly be tied to whitespace and could be put to other
   uses. In fact, this is what <comment> definitions are, and it possibly could
   be used in place of comments. It would probably be named "<ignored>", and
   would only be applicable to <token> definitions, and not to <subtoken> or
   <comment>. Additionally, we could make it applicable to non-token
   definitions used by the parsing phase.
 * Allow a definition named "whitespace" to be defined which defines
   whitespace. It would have to be marked <token>. If no such definitions was
   given, then a default would be generated which worked like
   `char.IsWhitespace`. 

Option #3 is a non-starter. The good part is that this is simple to understand.
The bad part is twofold: if a user wants a definition called 'whitespace' with
other meaning to them, they're out of luck; if a user misspells it as
'whietspace' or whatever, then it will not produce any errors or warnings,
very confusingly. That second one alone completely disqualifies this option.

Option #2 also won't work. If we only have `<ignored>`, there's no clear way to
say that a particular ignored token is whitespace and should therefor override
the default. The `<ignored>` might still be useful, except that we already have
`<comment>`. Perhaps we could replace `<comment>` with `<token, ignored>`, but
that's beyond the scope of the whitespace feature.

Option #1 then is the only one that will really work. We'll introduce a new
`<whitespace>` directive that allows the user to override the default set of
whitespace characters. It will be a 'tokenized' directive, alongside `<token>`,
`<subtoken>`, and `<comment>`, and will therefor need to be checked accordingly.
If more than one definition is marked with `<whitespace>`, that's an error. If
no definition is so marked, then a default will be added of the following form:
```
   <whitespace> $whitespace = [ \t\r\n];
```
Aditionally, the definition might include all unicode characters that are
covered by the `char.IsWhitespace` method:
```
   <whitespace> $whitespace = (
    [ \t\r\n] |                         /* basic whitespace */
    [\x000b\x000c] |                    /* additional ascii whitespace */
    [\x0085\x00a0] |                    /* additional unicode whitespace */
    [0x2029] |                          /* paragraph separator category */
    [0x2028] |                          /* line separator category */
    [0x1680\x2000\x2001\x2002\x2003] |  /* unicode space category */
    [\x2004\x2005\x2006\x2007\x2008] |  /* unicode space category */
    [\x2009\x200a\x202f\x205f\x3000]    /* unicode space category */
   );
```
For tokenized grammars, the `<whitespace>` directive implies `<token>`. Any
token generated by `Spanner` will simply be ignored, as is the case for
comments now.

Because the feature is about treating whitespace as tokens, the presence of the
`<whitespace>` directive in a non-tokenized grammar causes a warning and will
be ignored.

The `<mind whitespace>` directive will be removed, and its semantics will be on
all the time for all definitions.

Todo:
    Make it generate a default definition when not given
    Ignore all `<whitespace>` tokens
    Remove 'mine whitespace' from existing grammars
        Supergrammar -- tokenize?
        StringFormatterGrammar
    Make the semantics of `<mind whitespace>` always on
    Remove the `<mind whitespace>` directive
